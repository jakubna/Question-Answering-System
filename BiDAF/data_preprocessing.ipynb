{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_json(filename):\n",
    "    \"\"\"Loads JSON data from filename and returns\"\"\"\n",
    "    with open(filename) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sequence):\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in nltk.word_tokenize(sequence)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(out_file, line):\n",
    "    out_file.write(str(line.encode('utf8')) + str('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_word_loc_mapping(context, context_tokens):\n",
    "    \"\"\"\n",
    "    Return a mapping that maps from character locations to the corresponding token locations.\n",
    "    If we're unable to complete the mapping e.g. because of special characters, we return None.\n",
    "\n",
    "    Inputs:\n",
    "      context: string (unicode)\n",
    "      context_tokens: list of strings (unicode)\n",
    "\n",
    "    Returns:\n",
    "      mapping: dictionary from ints (character locations) to (token, token_idx) pairs\n",
    "        Only ints corresponding to non-space character locations are in the keys\n",
    "        e.g. if context = \"hello world\" and context_tokens = [\"hello\", \"world\"] then\n",
    "        0,1,2,3,4 are mapped to (\"hello\", 0) and 6,7,8,9,10 are mapped to (\"world\", 1)\n",
    "    \"\"\"\n",
    "    acc = '' # accumulator\n",
    "    current_token_idx = 0 # current word loc\n",
    "    mapping = dict()\n",
    "\n",
    "    for char_idx, char in enumerate(context): # step through original characters\n",
    "        if char != u' ' and char != u'\\n': # if it's not a space:\n",
    "            acc += char # add to accumulator\n",
    "            context_token = context_tokens[current_token_idx] # current word token\n",
    "            if acc == context_token: # if the accumulator now matches the current word token\n",
    "                syn_start = char_idx - len(acc) + 1 # char loc of the start of this word\n",
    "                for char_loc in range(syn_start, char_idx+1):\n",
    "                    mapping[char_loc] = (acc, current_token_idx) # add to mapping\n",
    "                acc = '' # reset accumulator\n",
    "                current_token_idx += 1\n",
    "\n",
    "    if current_token_idx != len(context_tokens):\n",
    "        return None\n",
    "    else:\n",
    "        return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_write(dataset, tier, out_dir):\n",
    "    \n",
    "    num_exs = 0 # number of examples written to file\n",
    "    num_mappingprob, num_tokenprob, num_spanalignprob, num_noanswer = 0, 0, 0, 0\n",
    "    examples = []\n",
    "    \n",
    "    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n",
    "\n",
    "        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
    "\n",
    "        for pid in range(len(article_paragraphs)):\n",
    "            context = article_paragraphs[pid]['context'] # string\n",
    "            # The following replacements are suggested in the paper\n",
    "\n",
    "            # BidAF (Seo et al., 2016)\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "\n",
    "            context_tokens = tokenize(context) # list of strings (lowercase)\n",
    "            context = context.lower()\n",
    "\n",
    "            qas = article_paragraphs[pid]['qas'] # list of questions\n",
    "\n",
    "\n",
    "            charloc2wordloc = get_char_word_loc_mapping(context, context_tokens) # charloc2wordloc maps the character location (int) of a context token to a pair giving (word (string), word loc (int)) of that token\n",
    "            if charloc2wordloc is None: # there was a problem\n",
    "                num_mappingprob += len(qas)\n",
    "                continue # skip this context example\n",
    "\n",
    "            # for each question, process the question and answer and write to file\n",
    "            for qn in qas:\n",
    "                try:\n",
    "                    # read the question text and tokenize\n",
    "                    question = qn['question'] # string\n",
    "                    question_tokens = tokenize(question) # list of strings\n",
    "\n",
    "                    # of the three answers, just take the first\n",
    "                    try:\n",
    "                        ans_text = qn['answers'][0]['text'].lower() # get the answer text\n",
    "                        ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
    "                    except:\n",
    "                        ans_text = qn['plausible_answers'][0]['text'].lower() # get the answer text\n",
    "                        ans_start_charloc = qn['plausible_answers'][0]['answer_start'] # answer start loc (character count)\n",
    "\n",
    "                    ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
    "\n",
    "                    # Check that the provided character spans match the provided answer text\n",
    "                    if context[ans_start_charloc:ans_end_charloc] != ans_text:\n",
    "                      # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
    "                      # We should upgrade to Python 3 next year!\n",
    "                      num_spanalignprob = num_spanalignprob + 1\n",
    "                      continue\n",
    "\n",
    "                    # get word locs for answer start and end (inclusive)\n",
    "                    ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
    "                    ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
    "                    assert ans_start_wordloc <= ans_end_wordloc\n",
    "\n",
    "                    # Check retrieved answer tokens match the provided answer text.\n",
    "                    # Sometimes they won't match, e.g. if the context contains the phrase \"fifth-generation\"\n",
    "                    # and the answer character span is around \"generation\",\n",
    "                    # but the tokenizer regards \"fifth-generation\" as a single token.\n",
    "                    # Then ans_tokens has \"fifth-generation\" but the ans_text is \"generation\", which doesn't match.\n",
    "                    ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
    "                    if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
    "                        num_tokenprob += 1\n",
    "                        continue # skip this question/answer pair\n",
    "                    \n",
    "                    examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)])))\n",
    "                    num_exs += 1\n",
    "                except:\n",
    "                    num_noanswer += 1\n",
    "\n",
    "    print (\"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n",
    "    print (\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n",
    "    print (\"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob)\n",
    "    print (\"Number of questions discarded due to not existing answer: \", num_noanswer)\n",
    "    print (\"Processed %i examples of total %i\\n\" % (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob))\n",
    "\n",
    "    \n",
    "    # shuffle examples\n",
    "    indices = list(range(len(examples)))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    out_dir = './data/preprocessed'\n",
    "\n",
    "    with open(os.path.join(out_dir, tier +'.context'), 'w') as context_file,  \\\n",
    "         open(os.path.join(out_dir, tier +'.question'), 'w') as question_file,\\\n",
    "         open(os.path.join(out_dir, tier +'.answer'), 'w') as ans_text_file, \\\n",
    "         open(os.path.join(out_dir, tier +'.span'), 'w') as span_file:\n",
    "\n",
    "        for i in indices:\n",
    "            (context, question, answer, answer_span) = examples[i]\n",
    "\n",
    "            # write tokenized data to file\n",
    "            write_to_file(context_file, str(context))\n",
    "            write_to_file(question_file, str(question))\n",
    "            write_to_file(ans_text_file, str(answer))\n",
    "            write_to_file(span_file, str(answer_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Preprocessing train: 100%|██████████| 442/442 [00:44<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  147\n",
      "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  1648\n",
      "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  37\n",
      "Number of questions discarded due to not existing answer:  0\n",
      "Processed 128487 examples of total 130319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dev: 100%|██████████| 35/35 [00:03<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  0\n",
      "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  128\n",
      "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  0\n",
      "Number of questions discarded due to not existing answer:  15\n",
      "Processed 11730 examples of total 11858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "train_filepath = './data/train-v2.0.json'\n",
    "dev_filepath = './data/dev-v2.0.json'\n",
    "\n",
    "preprocess_and_write(data_from_json(train_filepath), 'train', './data')\n",
    "preprocess_and_write(data_from_json(dev_filepath), 'dev', './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import argparse\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "from tqdm import *\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "\n",
    "_PAD = b\"<pad>\"\n",
    "_SOS = b\"<sos>\"\n",
    "_UNK = b\"<unk>\"\n",
    "_START_VOCAB = [_PAD, _SOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "SOS_ID = 1\n",
    "UNK_ID = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    code_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)))\n",
    "    vocab_dir = os.path.join(\"data\", \"squad\")\n",
    "    glove_dir = os.path.join(\"download\", \"dwr\")\n",
    "    source_dir = os.path.join(\"data\", \"squad\")\n",
    "    parser.add_argument(\"--source_dir\", default=source_dir)\n",
    "    parser.add_argument(\"--glove_dir\", default=glove_dir)\n",
    "    parser.add_argument(\"--vocab_dir\", default=vocab_dir)\n",
    "    parser.add_argument(\"--glove_dim\", default=100, type=int)\n",
    "    parser.add_argument(\"--random_init\", default=True, type=bool)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tokenizer(sentence):\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(\" \", space_separated_fragment))\n",
    "    return [w for w in words if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vocabulary(vocabulary_path):\n",
    "    # map vocab to word embeddings\n",
    "    if gfile.Exists(vocabulary_path):\n",
    "        rev_vocab = []\n",
    "        with gfile.GFile(vocabulary_path, mode=\"r\") as f:\n",
    "            rev_vocab.extend(f.readlines())\n",
    "        rev_vocab = [line.strip('\\n') for line in rev_vocab]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_glove(vocab_list, size=4e5, random_init=True):\n",
    "\n",
    "    save_path = './glove_vectors/_vectors'\n",
    "    glove_dim = 200\n",
    "    \n",
    "    if not gfile.Exists(save_path + \".npz\"):\n",
    "        glove_path = './glove_vectors/glove.6B.200d.txt'\n",
    "        if random_init:\n",
    "            glove = np.random.randn(len(vocab_list), glove_dim)\n",
    "        else:\n",
    "            glove = np.zeros((len(vocab_list), glove_dim))\n",
    "        found = 0\n",
    "        with open(glove_path, 'r') as fh:\n",
    "            for line in tqdm(fh, total=size):\n",
    "                array = line.lstrip().rstrip().split(\" \")\n",
    "                word = array[0]\n",
    "                vector = list(map(float, array[1:]))\n",
    "                if word in vocab_list:\n",
    "                    idx = vocab_list.index(word)\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "                if word.capitalize() in vocab_list:\n",
    "                    idx = vocab_list.index(word.capitalize())\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "                if word.upper() in vocab_list:\n",
    "                    idx = vocab_list.index(word.upper())\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "\n",
    "        print(\"{}/{} of word vocab have corresponding vectors in {}\".format(found, len(vocab_list), glove_path))\n",
    "        np.savez_compressed(save_path, glove=glove)\n",
    "        print(\"saved trimmed glove matrix at: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(vocabulary_path, data_paths, tokenizer=None):\n",
    "#     if not gfile.Exists(vocabulary_path):\n",
    "    print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, str(data_paths)))\n",
    "    vocab = {}\n",
    "    for path in data_paths:\n",
    "        with open(path, mode=\"rb\") as f:\n",
    "            print(f)\n",
    "            counter = 0\n",
    "            for line in f:\n",
    "                counter += 1\n",
    "                if counter % 100000 == 0:\n",
    "                    print(\"processing line %d\" % counter)\n",
    "                tokens = tokenizer(str(line)) if tokenizer else basic_tokenizer(str(line))\n",
    "                for w in tokens:\n",
    "                    if w in vocab:\n",
    "                        vocab[w] += 1\n",
    "                    else:\n",
    "                        vocab[w] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    print(\"Vocabulary size: %d\" % len(vocab_list))\n",
    "    with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n",
    "        for w in vocab_list:\n",
    "            vocab_file.write(str(w) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_token_ids(sentence, vocabulary, tokenizer=None):\n",
    "    if tokenizer:\n",
    "        words = tokenizer(str(sentence))\n",
    "    else:\n",
    "        words = basic_tokenizer(str(sentence))\n",
    "    return [vocabulary.get(w, UNK_ID) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
    "                      tokenizer=None):\n",
    "    if not gfile.Exists(target_path):\n",
    "        print(\"Tokenizing data in %s\" % data_path)\n",
    "        vocab, _ = initialize_vocabulary(vocabulary_path)\n",
    "        with gfile.GFile(data_path, mode=\"rb\") as data_file:\n",
    "            with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
    "                counter = 0\n",
    "                for line in data_file:\n",
    "                    counter += 1\n",
    "                    if counter % 5000 == 0:\n",
    "                        print(\"tokenizing line %d\" % counter)\n",
    "                    token_ids = sentence_to_token_ids(line, vocab, tokenizer)\n",
    "                    tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary ./vocabulary/vocab.dat from data ['./data/preprocessed/train.context', './data/preprocessed/train.question']\n",
      "<_io.BufferedReader name='./data/preprocessed/train.context'>\n",
      "processing line 100000\n",
      "<_io.BufferedReader name='./data/preprocessed/train.question'>\n",
      "processing line 100000\n",
      "Vocabulary size: 112838\n"
     ]
    }
   ],
   "source": [
    "source_dir = './data/preprocessed'\n",
    "vocab_dir = './vocabulary'\n",
    "\n",
    "vocab_path = pjoin(vocab_dir, \"vocab.dat\")\n",
    "train_path = pjoin(source_dir, \"train\")\n",
    "valid_path = pjoin(source_dir, \"dev\")\n",
    "dev_path = pjoin(source_dir, \"dev\")\n",
    "\n",
    "\n",
    "create_vocabulary(vocab_path,\n",
    "                  [pjoin(source_dir, \"train.context\"),\n",
    "                   pjoin(source_dir, \"train.question\")\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, rev_vocab = initialize_vocabulary(pjoin(vocab_dir, \"vocab.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Trim Distributed Word Representation =======\n",
    "# If you use other word representations, you should change the code below\n",
    "\n",
    "process_glove(rev_vocab, random_init=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import codecs\n",
    "\n",
    "file_path_train_span = '/home/andy/Documents/Moje/WEDT/projekt/my_code/data/preprocessed/train.span'\n",
    "file_path_dev_span = '/home/andy/Documents/Moje/WEDT/projekt/my_code/data/preprocessed/dev.span'\n",
    "\n",
    "def clear_span(file_path):\n",
    "    f = codecs.open(file_path_train_span, encoding='utf-8')\n",
    "    contents = f.read()\n",
    "\n",
    "\n",
    "    newcontents = contents.replace('b','')\n",
    "    newcontents = newcontents.replace('\\'', '')\n",
    "#     print(newcontents)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    x=open(file_path,\"w\")\n",
    "    x.write(newcontents)\n",
    "    x.close\n",
    "    \n",
    "clear_span(file_path_train_span)\n",
    "clear_span(file_path_dev_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Creating Dataset =========\n",
    "# We created our data files seperately\n",
    "# If your model loads data differently (like in bulk)\n",
    "# You should change the below code\n",
    "\n",
    "x_train_ids_path = train_path + \".ids.context\"\n",
    "y_train_ids_path = train_path + \".ids.question\"\n",
    "data_to_token_ids(train_path + \".context\", x_train_ids_path, vocab_path)\n",
    "data_to_token_ids(train_path + \".question\", y_train_ids_path, vocab_path)\n",
    "\n",
    "x_dis_path = valid_path + \".ids.context\"\n",
    "y_ids_path = valid_path + \".ids.question\"\n",
    "data_to_token_ids(dev_path + \".context\", x_dis_path, vocab_path)\n",
    "data_to_token_ids(dev_path + \".question\", y_ids_path, vocab_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
